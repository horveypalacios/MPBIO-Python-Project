{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6544a567-00f0-45bc-8311-3d1f290d3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse as ap\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn import decomposition\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d39a63-f5e8-4a97-b21c-960177157650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class class_metrics:\n",
    "\tdef __init__(self):\n",
    "\t\tself.accuracy = []\n",
    "\t\tself.f1 = []\n",
    "\t\tself.precision = []\n",
    "\t\tself.recall = []\n",
    "\t\tself.auc = []\n",
    "\t\tself.roc_curve = []\n",
    "\t\tself.confusion_matrix = []\n",
    "\n",
    "\t\t# self.sensitivity = []\n",
    "\t\t# self.specificity = []\n",
    "\t\t# mcc  \n",
    "        \n",
    "class class_params:\n",
    "\tdef __init__(self):\n",
    "\t\tself.learner_type = []\n",
    "\t\tself.feature_selection = []\n",
    "\t\tself.cv_folds = []\n",
    "\t\tself.cv_grid = []\n",
    "\t\tself.cv_scoring = []\n",
    "\t\tself.fs_grid = []\n",
    "\n",
    "\t\t#self.refine = []\n",
    "\t\t#self.refine_grid = []\n",
    "\n",
    "\n",
    "class feature_importance:\n",
    "\tdef __init__(self, feat, p):\n",
    "\t\tself.feat_sel = feat\n",
    "\t\tself.imp = np.array([p]*len(feat))\n",
    "\n",
    "\n",
    "def compute_feature_importance(el, feat, feat_sel, ltype):\n",
    "\tfi = feature_importance(feat, 0.0)\n",
    "\tif ltype == 'rf':\n",
    "\t\tt = el.feature_importances_\n",
    "\telif (ltype == 'lasso') | (ltype == 'enet'):\n",
    "\t\tt = abs(el.coef_)/sum(abs(el.coef_))\n",
    "\telse:\n",
    "\t\tt = [1.0/len(feat_sel)]*len(feat_sel)\n",
    "\tti = [feat.index(s) for s in feat_sel]\n",
    "\tfi.imp[ti] = t\n",
    "\n",
    "\tt = sorted(range(len(t)), key=lambda s: t[s], reverse=True)\n",
    "\tfi.feat_sel = [feat[ti[s]] for s in t if fi.imp[ti[s]] != 0]\n",
    "\n",
    "\treturn fi\n",
    "\n",
    "\n",
    "def plot_pca(f, l, feat_sel):\n",
    "\tf = f[feat_sel].values\t\n",
    "\tl = l.values.flatten().astype('int')\n",
    "\t\n",
    "\tpca = decomposition.PCA(n_components=2)\n",
    "\tpca = pca.fit(f)\n",
    "\tft = pca.transform(f)\n",
    "\n",
    "\tfig, ax = plt.subplots()\n",
    "\tfor i in np.unique(l):\n",
    "\t\tax.scatter(ft[l==i, 0], ft[l==i, 1], c=l[l==i], s=200, \\\n",
    "\t\tcmap=plt.cm.jet, vmin=min(l), vmax=max(l), edgecolors='None', alpha=0.6)\n",
    "\t\tax.text(ft[l==i, 0].mean(), ft[l==i, 1].mean(), 'Class ' + str(i), \\\n",
    "\t\thorizontalalignment='center', bbox=dict(alpha=0.5, edgecolor='w', facecolor='w'))\n",
    "\n",
    "\tax.set_xlabel('PC1 (' + str(np.round(100*pca.explained_variance_ratio_[0], decimals=2)) + '%)')\n",
    "\tax.set_ylabel('PC2 (' + str(np.round(100*pca.explained_variance_ratio_[1], decimals=2)) + '%)')\n",
    "\tax.set_xlim([min(ft[:, 0]),max(ft[:, 0])])\n",
    "\tax.set_ylim([min(ft[:, 1]),max(ft[:, 1])])\t\n",
    "\t\n",
    "\tfig.savefig(par['out_f'] + '_pca.' + par['figure_extension'], bbox_inches='tight')\n",
    "\n",
    "\n",
    "def read_params():\n",
    "\tparser = ap.ArgumentParser(\\\n",
    "\t    description='MetAML - Metagenomic prediction Analysis based on Machine Learning')\n",
    "\targ = parser.add_argument\n",
    "\targ( 'inp_f', metavar='INPUT_FILE', nargs='?', default=sys.stdin, type=str\\\n",
    "\t\t, help=\"the input dataset file [stdin if not present]\")\n",
    "\targ( 'out_f', metavar='OUTPUT_FILE', nargs='?', default=None, type=str\\\n",
    "\t\t, help=\"the output file [stdout if not present]\")\n",
    "\targ( '-z','--feature_identifier', default='k__', type=str, help=\"the feature identifier\\n\")\n",
    "\targ( '-d','--define', type=str, help=\"define the classification problem\\n\")\n",
    "\targ( '-t','--target', type=str, help=\"define the target domain\\n\")\n",
    "\targ( '-u','--unique', type=str, help=\"the unique samples to select\\n\")\n",
    "\targ( '-b','--label_shuffling', action='store_true', help=\"label shuffling\\n\")\n",
    "\n",
    "\targ( '-r','--runs_n', default=20, type=int, help=\"the number of runs\\n\")\n",
    "\targ( '-p','--runs_cv_folds', default=10, type=int, help=\"the number of cross-validation folds per run\\n\")\n",
    "\targ( '-w','--set_seed', action='store_true', help=\"setting seed\\n\")\n",
    "\targ( '-l','--learner_type', choices=['rf','lsvm','svm','lasso','enet'], default='rf', help='the type of learner/classifier\\n')\n",
    "\targ( '-i','--feature_selection', choices=['lasso','enet'], help=\"the type of feature selection\\n\")\n",
    "\targ( '-f','--cv_folds', type=int, help=\"the number of cross-validation folds for model selection\\n\")\n",
    "\targ( '-g','--cv_grid', type=str, help=\"the parameter grid for model selection\\n\")\n",
    "\targ( '-s','--cv_scoring', default='roc_auc', type=str, help=\"the scoring function for model selection\\n\")\n",
    "\targ( '-j','--fs_grid', type=str, help=\"the parameter grid for feature selection\\n\")\n",
    "\n",
    "\targ( '-re','--refine', default='rf', type=str, choices=['rf','svm'], \\\n",
    "\t    help='after selecting features with random forest, you can try also svm on reduced sets (not implemented)')\n",
    "\t## random forest options\n",
    "\targ( '-c','--rf_criterion', type=str, choices=['gini', 'entropy'], default='entropy', \\\n",
    "\t    help='Impurity criterion (random forest)')\n",
    "\targ( '-mf','--rf_max_features'\\\n",
    "\t    , choices=['0.001', '0.01', '0.1', '0.2', '0.3', '0.5', '0.4', '0.6', '1.0'\\\n",
    "\t    , '100', 'auto', 'sqrt', '0.33', None, 'log2'], default=0.3, \\\n",
    "\t    help='Feature sample/percentage (random forest)')\n",
    "\targ( '-nt','--number_of_trees', type=int, default=1000, help='# of estimator trees (random forest)')\n",
    "\targ( '-nsl','--number_sample_per_leaf', type=int, default=1, help='minimum # sample per leaf (random forest)')\n",
    "\targ( '-oob','--oob_score', action='store_true', help='Enable out-of-bag choice in random forest')\n",
    "\targ( '-df','--disable_features', action='store_true', help='Doesn\\'t perform the features selection' +\\\n",
    "             'which follows std random forest (random forest)')\n",
    "\targ( '-cc', '--choose_cut', type=str, default=None, \\\n",
    "\t    help='comma-separated list of numbers which will substitute the std features cuts (10,20,...,150)')\n",
    "\targ( '-wc', '--weight_classes', default=None, type=float, nargs=2)\n",
    "\n",
    "\targ( '-hv','--how_verbose', default=1, type=int, choices=[0,1,2])\n",
    "\targ( '-e','--figure_extension', default='png', type=str, help=\"the extension of output figure\\n\")\n",
    "\targ( '-nc', '--ncores', type=int, default=10, help='-1 set to all the available cores.')\n",
    "\targ( '-lk','--linear_kernel', action='store_true')\n",
    "\targ( '--no_norm', action='store_true', help='Disable per-sample normalisation')\n",
    "\n",
    "\targ( '-ovec', '--objective_vector', type=str, default=None, \\\n",
    "\t    help='Like a DEFINED, but it only replaces the final vector on which to predict on')\n",
    "\n",
    "\treturn vars(parser.parse_args())\n",
    "\n",
    "\n",
    "\n",
    "def save_average_feature_importance(fi, feat):\n",
    "\tfi_ave = feature_importance(feat, 0.0)\n",
    "\n",
    "\tt = [s.imp for s in fi]\n",
    "\tfi_ave_std = np.std(t, axis=0)\n",
    "\tt = np.mean(t, axis=0)\n",
    "\tfi_ave.imp = t\n",
    "\tt = sorted(range(len(t)), key=lambda s: t[s], reverse=True)\n",
    "\tfi_ave.feat_sel = [feat[s] for s in t if fi_ave.imp[s] != 0]\n",
    "\n",
    "\tfidout.write('Feature importance (ranking, name, average, std)\\n')\n",
    "\t[fidout.write(str(s) + '\\t' + feat[t[s]] + '\\t' + str(fi_ave.imp[t[s]]) \\\n",
    "\t\t+ '\\t' + str(fi_ave_std[t[s]]) + '\\n') for s in range(len(t))]\n",
    "\n",
    "\treturn fi_ave\n",
    "\n",
    "\n",
    "def save_results(l, l_es, p_es, i_tr, i_u, nf, runs_n, runs_cv_folds):\n",
    "\n",
    "\tn_clts = len(np.unique(l.values.flatten().astype('int')))\n",
    "\tcm = class_metrics()\n",
    "\n",
    "\tif par['out_f']:\n",
    "\t\tfidoutes.write('#features\\t' + str(nf) + '\\n')\n",
    "\t\tif n_clts == 2:\n",
    "\t\t\tfidoutroc.write('#features\\t' + str(nf) + '\\n')\t\n",
    "\n",
    "\tfor j in range( runs_n * runs_cv_folds ):\n",
    "\t\tl_ = pd.DataFrame([l.loc[i] for i in l[~i_tr[j] & i_u[j//runs_cv_folds]].index]).values.flatten().astype('int')\n",
    "\n",
    "\t\tl_es_ = l_es[j].values.flatten().astype('int')\n",
    "\t\tif (lp.learner_type == 'rf') | (lp.learner_type.endswith('svm')):\n",
    "\t\t\tp_es_pos_ = p_es[j].loc[:,1].values\n",
    "\t\telse:\n",
    "\t\t\tp_es_pos_ = p_es[j].loc[:,0].values\n",
    "\t\tii_ts_ = [i for i in range(len(i_tr[j])) if i_tr[j][i]==False]\n",
    "\n",
    "\t\tcm.accuracy.append(metrics.accuracy_score(l_, l_es_))\n",
    "\t\tcm.f1.append(metrics.f1_score(l_, l_es_, pos_label=None, average='weighted'))\n",
    "\t\tcm.precision.append(metrics.precision_score(l_, l_es_, pos_label=None, average='weighted'))\n",
    "\t\tcm.recall.append(metrics.recall_score(l_, l_es_, pos_label=None, average='weighted'))\n",
    "\n",
    "\t\tif len(np.unique(l_)) in [ n_clts, n_clts-1 ]:\n",
    "\t\t\tif n_clts == 2:\n",
    "\n",
    "\t\t\t\ttry: cm.auc.append(metrics.roc_auc_score(l_, p_es_pos_))\n",
    "\t\t\t\texcept ValueError: cm.auc.append(0.0)\n",
    "\n",
    "\t\t\t\tcm.roc_curve.append(metrics.roc_curve(l_, p_es_pos_))\n",
    "\t\t\t\tfidoutroc.write('run/fold\\t' + str(j//runs_cv_folds) + '/' + str(j%runs_cv_folds) + '\\n')\n",
    "\t\t\t\tfor i in range(len(cm.roc_curve[-1])):\n",
    "\t\t\t\t\tfor i2 in range(len(cm.roc_curve[-1][i])):\n",
    "\t\t\t\t\t\tfidoutroc.write(str(cm.roc_curve[-1][i][i2]) + '\\t')\n",
    "\t\t\t\t\tfidoutroc.write('\\n')\n",
    "\n",
    "\t\t\tcf = metrics.confusion_matrix(l_, l_es_, labels=np.unique(l.astype('int')))\n",
    "\t\t\tcm.confusion_matrix.append(cf)\n",
    "\t\t\t##\tmetrics.confusion_matrix(l_, l_es_, labels=np.unique(l.astype('int'))))\n",
    "\t\t\t#cm.sensitivity.append( cf[1,1] / float( cf[1,1] + cf[1,0] ) )\n",
    "\t\t\t#cm.specificity.append( cf[0,0] / float( cf[0,0] + cf[0,1] ) )\n",
    "\n",
    "\t\tif par['out_f']:\n",
    "\t\t\tfidoutes.write('run/fold\\t' + str(j//runs_cv_folds) + '/' + str(j%runs_cv_folds))\n",
    "\t\t\tfidoutes.write('\\ntrue labels\\t')\n",
    "\t\t\t[fidoutes.write(str(i)+'\\t') for i in l_]\n",
    "\t\t\tfidoutes.write('\\nestimated labels\\t')\n",
    "\t\t\t[fidoutes.write(str(i)+'\\t') for i in l_es_]\n",
    "\t\t\tif n_clts <= 2:\n",
    "\t\t\t\tfidoutes.write('\\nestimated probabilities\\t')\n",
    "\t\t\t\t[fidoutes.write(str(i)+'\\t') for i in p_es_pos_]\n",
    "\t\t\tfidoutes.write('\\nsample index\\t')\t\t\t\n",
    "\t\t\t[fidoutes.write(str(i)+'\\t') for i in ii_ts_]\n",
    "\t\t\tfidoutes.write('\\n')\n",
    "\n",
    "\tfidout.write('#samples\\t' + str(sum(sum(i_u))/len(i_u)))\n",
    "\tfidout.write('\\n#features\\t' + str(nf))\n",
    "\tfidout.write('\\n#runs\\t' + str(runs_n))\n",
    "\tfidout.write('\\n#runs_cv_folds\\t' + str(runs_cv_folds))\t\n",
    "\n",
    "\tfidout.write('\\naccuracy\\t' + str(np.mean(cm.accuracy)) + '\\t' + str(np.std(cm.accuracy)))\n",
    "\tfidout.write('\\nf1\\t' + str(np.mean(cm.f1)) + '\\t' + str(np.std(cm.f1)))\n",
    "\n",
    "\tfidout.write('\\nprecision\\t' + str(np.mean(cm.precision)) + '\\t' + str(np.std(cm.precision)))\n",
    "\tfidout.write('\\nrecall\\t' + str(np.mean(cm.recall)) + '\\t' + str(np.std(cm.recall)))\n",
    "\n",
    "        #fidout.write('\\nsensitivity\\t' + str(np.mean(cm.sensitivity)) + '\\t' + str(np.std(cm.sensitivity)))\n",
    "        #fidout.write('\\nspecificity\\t' + str(np.mean(cm.specificity)) + '\\t' + str(np.std(cm.specificity)))\n",
    "\n",
    "\tif n_clts == 2:\n",
    "\t\tfidout.write('\\nauc\\t' + str(np.mean(cm.auc)) + '\\t' + str(np.std(cm.auc)))\n",
    "\telse:\n",
    "\t\tfidout.write('\\nauc\\t[]\\t[]')\n",
    "\tfidout.write('\\nconfusion matrix')\n",
    "\n",
    "\tif len(cm.confusion_matrix) > 0:\n",
    "\t\tfor i in range(len(cm.confusion_matrix[0])):\n",
    "\t\t\tfor i2 in range(len(cm.confusion_matrix[0][i])):\n",
    "\t\t\t\tfidout.write('\\t' + str(np.sum([cm.confusion_matrix[j][i][i2] for j in range(len(cm.confusion_matrix))])))\n",
    "\t\t\tfidout.write('\\n')\n",
    "\n",
    "\telse:\n",
    "\t\tfidout.write('\\n')\n",
    "\n",
    "\treturn cm\n",
    "\n",
    "\n",
    "\n",
    "def set_class_params(args, l):\n",
    "\tlp = class_params()\n",
    "\n",
    "\tif par['learner_type']:\n",
    "\t\tlp.learner_type = par['learner_type']\n",
    "\t\tif (max(l.values.flatten().astype('int'))>1) & (lp.learner_type != 'svm'):\n",
    "\t\t\tlp.learner_type = 'rf'\t\n",
    "\telse:\n",
    "\t\tlp.learner_type = 'rf'\n",
    "\n",
    "\tlp.refine = par['refine']\n",
    "\tlp.refine_grid = [{'C': [1,1000], 'kernel':['linear']}, {'C': [1, 1000], 'gamma': [10, 1, 0.1, 0.01, 0.001, 0.0001], 'kernel':['rbf']}]\n",
    "\n",
    "\tif par ['feature_selection']:\n",
    "\t\tlp.feature_selection = par['feature_selection']\n",
    "\telse:\n",
    "\t\tlp.feature_selection = 'none'\n",
    "\n",
    "\tif par['cv_folds']:\n",
    "\t\tlp.cv_folds = int(par['cv_folds'])\n",
    "\telse:\n",
    "\t\tlp.cv_folds = 10\n",
    "\n",
    "\tif par['cv_grid']:\n",
    "\t\tlp.cv_grid = eval(par['cv_grid'])\n",
    "\n",
    "\telif lp.learner_type == 'rf':\n",
    "\t\tlp.cv_grid = [{'max_features': [0.33, 'auto']}]\n",
    "\t#\tlp.cv_grid = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 16, 32]\n",
    "\t\t# 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 125, 150, 175, 200]\n",
    "\n",
    "\telif lp.learner_type == 'svm':\n",
    "\t\t#if par['feature_identifier'] == 'k__':\n",
    "\t\tlp.cv_grid = [ {'C': [2**s for s in range(-5,16,2)], 'kernel': ['linear']}, \\\n",
    "\t\t    {'C': [2**s for s in range(-5,16,2)], 'gamma': [2**s for s in range(3,-15,-2)], 'kernel': ['rbf']} ]\n",
    "\t\t#\tlp.cv_grid = [ {'C': [1, 10, 100, 1000], 'kernel': ['linear']}, \\\n",
    "\t\t#   {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']} ]\n",
    "\n",
    "\telif lp.learner_type == 'lsvm':\n",
    "\t\tlp.cv_grid = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']}]\n",
    "\n",
    "\telif lp.learner_type == 'lasso':\n",
    "\t\tlp.cv_grid = [np.logspace(-4, -0.5, 50)]\n",
    "\telif lp.learner_type == 'enet':\n",
    "\t\tlp.cv_grid = [np.logspace(-4, -0.5, 50), [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1.0]]\n",
    "\n",
    "\tif par['fs_grid']:\n",
    "\t\tlp.fs_grid = eval(par['fs_grid'])\n",
    "\telif lp.feature_selection == 'lasso':\n",
    "\t\tlp.fs_grid = [np.logspace(-4, -0.5, 50)]\n",
    "\telif lp.feature_selection == 'enet':\n",
    "\t\tlp.fs_grid = [np.logspace(-4, -0.5, 50), [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1.0]]\n",
    "\t\n",
    "\tlp.cv_scoring = par['cv_scoring']\n",
    "\n",
    "\treturn lp\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\tpar = read_params()\n",
    " \n",
    "\tif par['rf_max_features'] in ['0.001', '0.01', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.33','100', '1.0']:\n",
    "\t\tpar['rf_max_features'] = float(par['rf_max_features'])\n",
    "\n",
    "\tf = pd.read_csv(par['inp_f'], sep='\\t', header=None, index_col=0 ) #, dtype=unicode)\n",
    "\tf = f.T\n",
    "\n",
    "\tif par['out_f']:\n",
    "\t\tfidout = open(par['out_f'] + '.txt','w')\n",
    "\t\tfidoutes = open(par['out_f'] + '_estimations.txt','w')\n",
    "\t\tfidoutroc = open(par['out_f'] + '_roccurve.txt','w')\n",
    "\telse:\n",
    "\t\tfidout = sys.stdout\n",
    "\n",
    "\tif par['unique']:\n",
    "\t\tpf = pd.DataFrame([s.split(':') for s in par['unique'].split(',')])\n",
    "\n",
    "\tif par['define']:\n",
    "\t\td = pd.DataFrame([s.split(':') for s in par['define'].split(',')])\n",
    "\t\tl = pd.DataFrame([0]*len(f))\n",
    "\n",
    "\t\tif par['objective_vector']: \n",
    "\t\t\td_ob = pd.DataFrame([s.split(':') for s in par['objective_vector'].split(',')])\n",
    "\t\t\tl_ob =  pd.DataFrame([0]*len(f))\n",
    "\n",
    "\t\tfor i in range(len(d)):\n",
    "\t\t\tl[(f[d.iloc[i,1]].isin(d.iloc[i,2:])).tolist()] = d.iloc[i,0]\n",
    "\t\t\tif par['objective_vector']:\n",
    "\t\t\t\tl_ob[(f[d_ob.iloc[i,1]].isin(d_ob.iloc[i,2:])).tolist()] = d_ob.iloc[i,0]\n",
    "\n",
    "\telse:\n",
    "\t\tle = prep.LabelEncoder()\n",
    "\t\tle.fit(f.iloc[:,0])\n",
    "\t\tl = pd.DataFrame(le.transform(f.iloc[:,0])).astype('int')\n",
    "\t\tl_ob = pd.DataFrame(le.transform(f.iloc[:,0])).astype('int')\n",
    "\n",
    "\truns_n = par['runs_n']\n",
    "\n",
    "\tif par ['target']:\n",
    "\t\truns_cv_folds = 1\n",
    "\telse:\n",
    "\t\truns_cv_folds = par['runs_cv_folds']\n",
    "\n",
    "\ti_tr = pd.DataFrame(True, index=range(len(f.index)), columns=range(runs_n*runs_cv_folds))\n",
    "\tif par['objective_vector']:\n",
    "\t\ti_tr_ob = pd.DataFrame(True, index=range(len(f.index)), columns=range(runs_n*runs_cv_folds))\n",
    "\n",
    "\tif par['target']:\n",
    "\t\ti_u = pd.DataFrame(True, index=range(len(f.index)), columns=range(runs_n))\n",
    "\telse:\n",
    "\t\tif par['unique']:\n",
    "\t\t\ti_u = pd.DataFrame(False, index=range(len(f.index)), columns=range(runs_n))\n",
    "\t\t\tmeta_u = [s for s in f.columns if s in pf.iloc[0,0:].tolist()]\n",
    "\t\telse:\n",
    "\t\t\ti_u = pd.DataFrame(True, index=range(len(f.index)), columns=range(runs_n))\n",
    "\n",
    "\tfor j in range(runs_n):\n",
    "\t\tif par['set_seed']:\n",
    "\t\t\tnp.random.seed(j)\n",
    "\t\tif par['target']:\n",
    "\t\t\tt = pd.DataFrame([s.split(':') for s in par['target'].split(',')])\n",
    "\t\t\tfor i in range(len(t)):\n",
    "\t\t\t\ti_tr[j][(f[t.iloc[i,0]].isin(t.iloc[i,1:])).tolist()] = False\n",
    "\t\t\t\tif par['objective_vector']:\n",
    "\t\t\t\t\ti_tr_ob[j][(f[t.iloc[i,0]].isin(t.iloc[i,1:])).tolist()] = False\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tif par['unique']:\n",
    "\t\t\t\tii_u = [s-1 for s in (f.loc[np.random.permutation(f.index),:].drop_duplicates(meta_u)).index]\n",
    "\t\t\t\ti_u[j][ii_u] = True\n",
    "\t\t\telse:\n",
    "\t\t\t\tii_u = range(len(f.index))\n",
    "\n",
    "\n",
    "\t\t\tskf = StratifiedKFold(n_splits = runs_cv_folds, shuffle = True, random_state = (j if par['set_seed'] else None))\n",
    "\t\t\tallowed_values = np.array(l.iloc[i_u.values.T[j], 0].values, dtype=np.float64)\n",
    "\n",
    "\t\t\tskf_split = skf.split(np.array([[0, 0] for q in range(len(allowed_values))], dtype=np.float64), allowed_values)\n",
    "\t\t\ttest_folds = [tf[1] for tf in skf_split] # [train_index,test_index in skf_split]]\n",
    "\n",
    "\t\t\tfor i in range(runs_cv_folds):\n",
    "\t\t\t\tfor s in test_folds[i]:\n",
    "\t\t\t\t\ti_tr[j*runs_cv_folds + i ][ii_u[ s ]] = False\n",
    "\n",
    "\ti_tr = i_tr.values.T\n",
    "\ti_u = i_u.values.T\n",
    "\n",
    "\tif par['label_shuffling']:\n",
    "\t\tnp.random.shuffle(l.values)\n",
    "\n",
    "\tfeat = [s for s in f.columns if sum([s2 in s for s2 in par['feature_identifier'].split(':')])>0]\n",
    "\tif 'unclassified' in f.columns: feat.append('unclassified')\n",
    "\tf = f.loc[:, feat].astype('float')\n",
    "\n",
    "\tif not par['no_norm']:\n",
    "\t\tf = (f-f.min())/(f.max()-f.min())\n",
    "\n",
    "\tlp = set_class_params(sys.argv, l)\n",
    "\n",
    "\tfi = []\n",
    "\tclf = []\n",
    "\tp_es = []\n",
    "\tl_es = []\n",
    "\n",
    "\tglobal_time = time.time()\n",
    "\n",
    "\tfor j in range(runs_n*runs_cv_folds):\n",
    "\t\tstart_run_time = time.time()\n",
    "\t\tfi.append(feature_importance(feat, 1.0/len(feat)))\n",
    "\n",
    "\t\tif lp.feature_selection == 'lasso':\n",
    "\t\t\tfi[j] = compute_feature_importance(LassoCV(\\\n",
    "\t\t\talphas=lp.fs_grid[0], cv=lp.cv_folds, n_jobs=-1).fit(\\\n",
    "\t\t\tf.loc[i_tr[j] & i_u[j//runs_cv_folds], fi[j].feat_sel].values\\\n",
    "\t\t\t, l[i_tr[j] & i_u[j//runs_cv_folds]].values.flatten().astype('int'))\\\n",
    "\t\t\t, feat, fi[j].feat_sel, lp.feature_selection)\n",
    "\n",
    "\t\telif lp.feature_selection == 'enet':\n",
    "\t\t\tfi[j] = compute_feature_importance(ElasticNetCV(\\\n",
    "\t\t\talphas=lp.fs_grid[0], l1_ratio=lp.fs_grid[1], cv=lp.cv_folds, n_jobs=-1).fit(\\\n",
    "\t\t\tf.loc[i_tr[j] & i_u[j//runs_cv_folds], fi[j].feat_sel].values\\\n",
    "\t\t\t, l[i_tr[j] & i_u[j//runs_cv_folds]].values.flatten().astype('int'))\\\n",
    "\t\t\t, feat, fi[j].feat_sel, lp.feature_selection)\t\t\t\n",
    " \n",
    "\n",
    "\t\tif lp.learner_type == 'rf':\n",
    "\t\t\tif not par['rf_max_features']:\n",
    "\t\t\t\thypers = GridSearchCV(\\\n",
    "\t\t\t\t\tRandomForestClassifier(\\\n",
    "\t\t\t\t\t  n_estimators=par['number_of_trees']\\\n",
    "\t\t\t\t\t, min_samples_leaf=par['number_sample_per_leaf']\\\n",
    "\t\t\t\t\t, criterion='entropy'\\\n",
    "\t\t\t\t\t, max_depth=None\\\n",
    "\t\t\t\t\t, min_samples_split=2\\\n",
    "\t\t\t\t\t, n_jobs=10\\\n",
    "\t\t\t\t\t, verbose=par['how_verbose'], oob_score=par['oob_score'])\\\n",
    "\t\t\t\t\t, lp.cv_grid, cv=StratifiedKFold(\\\n",
    "\t\t\t\t\t  l.iloc[i_tr[j] & i_u[j/runs_cv_folds],0], lp.cv_folds, shuffle=True)\\\n",
    "\t\t\t\t\t, scoring=lp.cv_scoring, refit=False).fit(\\\n",
    "\t\t\t\tf.loc[i_tr[j] & i_u[j//runs_cv_folds], fi[j].feat_sel].values\\\n",
    "\t\t\t\t, l[i_tr[j] & i_u[j//runs_cv_folds]].values.flatten().astype('int'))\n",
    "\t\n",
    "\t\t\t\tclf.append(RandomForestClassifier(\\\n",
    "\t\t\t\t\tn_estimators=par['number_of_trees'], criterion=par['rf_criterion']\\\n",
    "\t\t\t\t\t, max_features=hypers.best_params_['max_features']\\\n",
    "\t\t\t\t\t, oob_score=par['oob_score'], max_depth=None, min_samples_split=2\\\n",
    "\t\t\t\t\t, n_jobs=par['ncores'], verbose=par['how_verbose']\\\n",
    "\t\t\t\t\t, min_samples_leaf=par['number_sample_per_leaf']).fit(\\\n",
    "\t\t\t\tf.loc[i_tr[j] & i_u[j//runs_cv_folds], fi[j].feat_sel].values\\\n",
    "\t\t\t\t, l[i_tr[j] & i_u[j//runs_cv_folds]].values.flatten().astype('int')))\t\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tclf.append(RandomForestClassifier(\\\n",
    "\t\t\t\t\tn_estimators=par['number_of_trees']\\\n",
    "\t\t\t\t\t, criterion=par['rf_criterion']\\\n",
    "\t\t\t\t\t, max_features=par['rf_max_features']\\\n",
    "\t\t\t\t\t, oob_score=par['oob_score'], max_depth=None\\\n",
    "\t\t\t\t\t, min_samples_split=2, n_jobs=par['ncores']\\\n",
    "\t\t\t\t\t, verbose=par['how_verbose']\\\n",
    "\t\t\t\t\t, min_samples_leaf=par['number_sample_per_leaf']\\\n",
    "\t\t\t\t\t, class_weight='balanced').fit(\\\n",
    "                                f.loc[i_tr[j] & i_u[j//runs_cv_folds], fi[j].feat_sel].values\\\n",
    "                                , l[i_tr[j] & i_u[j//runs_cv_folds]].values.flatten().astype(int)))\n",
    "\n",
    "\t\telif lp.learner_type.endswith('svm'):\n",
    "\t\t\tclf.append(GridSearchCV(\\\n",
    "                                SVC(probability=True, verbose=bool(par['how_verbose']))\\\n",
    "                                , lp.cv_grid, cv=StratifiedKFold(l.iloc[i_tr[j] & i_u[j/runs_cv_folds],0]\\\n",
    "\t\t\t\t, lp.cv_folds, shuffle=True)\\\n",
    "                                , scoring=lp.cv_scoring)\\\n",
    "                        .fit(f.loc[i_tr[j] & i_u[j//runs_cv_folds], fi[j].feat_sel].values\\\n",
    "\t\t\t, l[i_tr[j] & i_u[j//runs_cv_folds]].values.flatten().astype('int')))\n",
    "\n",
    "\t\telif lp.learner_type == 'lasso':\n",
    "\t\t\tclf.append(LassoCV(alphas=lp.cv_grid[0], cv=lp.cv_folds, n_jobs=-1).fit(\\\n",
    "\t\t\t\tf.loc[i_tr[j] & i_u[j//runs_cv_folds], fi[j].feat_sel].values\\\n",
    "\t\t\t\t, l[i_tr[j] & i_u[j//runs_cv_folds]].values.flatten().astype('int')))\n",
    "\n",
    "\t\telif lp.learner_type == 'enet':\n",
    "\t\t\tclf.append(ElasticNetCV(\\\n",
    "\t\t\talphas=lp.cv_grid[0], l1_ratio=lp.cv_grid[1], cv=lp.cv_folds, n_jobs=-1).fit(\\\n",
    "\t\t\tf.loc[i_tr[j] & i_u[j//runs_cv_folds], fi[j].feat_sel].values\\\n",
    "\t\t\t, l[i_tr[j] & i_u[j//runs_cv_folds]].values.flatten().astype('int')))\n",
    "\n",
    "\n",
    " \n",
    "\t\tif (lp.learner_type == 'rf') | (lp.learner_type.endswith('svm')):                        \n",
    "\t\t\tp_es.append(pd.DataFrame(clf[j].predict_proba(f.loc[~i_tr[j] & i_u[j//runs_cv_folds], fi[j].feat_sel].values)))\n",
    "\t\t\tl_es.append(pd.DataFrame([list(p_es[j].iloc[i,:]).index(max(p_es[j].iloc[i,:])) for i in range(len(p_es[j]))]))\n",
    "\n",
    "\t\telif lp.learner_type == 'precomp':\n",
    "\t\t\tp_es.append(pd.DataFrame(f.loc[~i_tr[j] & i_u[j//runs_cv_folds], fi[j].feat_sel].values))\n",
    "\t\t\tl_es.append(pd.DataFrame([int(p_es[j].iloc[i]>0.5) for i in range(len(p_es[j]))]))\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tp_es.append(pd.DataFrame(clf[j].predict(f.loc[~i_tr[j] & i_u[j//runs_cv_folds], fi[j].feat_sel].values)))\n",
    "\t\t\tl_es.append(pd.DataFrame([int(p_es[j].iloc[i]>0.5) for i in range(len(p_es[j]))]))\n",
    "\n",
    "\t\telapsed_run_time = time.time() - start_run_time\n",
    "\t\tif par['how_verbose'] > 0:\n",
    "\t\t\tprint('%i run-time: %.4f sec.' %(j, float(elapsed_run_time)))\n",
    "\t\n",
    "\tcm = save_results(l if not par['objective_vector'] else l_ob, \\\n",
    "\t\tl_es, p_es, i_tr if not par['objective_vector'] else i_tr_ob, \\\n",
    "\t\ti_u, len(feat), runs_n, runs_cv_folds)\n",
    "\n",
    "\tglobal_elapsed = time.time() - global_time\n",
    "\tif par['how_verbose'] > 0:\n",
    "\t\tprint ('global-time: %.4f sec.' %(global_elapsed))\n",
    "\n",
    "\n",
    "\t## from here on, if you used ranfom forest and you didn't specify\n",
    "\t## disable_features (which is useful in case of huge of very large database)\n",
    "\t## you have extracted a ranking of the most predictive features which\n",
    "\t## is averaged over '# folds * # runs' cicles. The testing sets are at\n",
    "\t## each cycle excluded, so you can use this set of selected features\n",
    "\t## without worrying\n",
    "\tif lp.learner_type == 'rf':\n",
    "\t\tif not par['disable_features']:\n",
    "\t\t\tfi_f = []\n",
    "\t\t\tfor j in range(runs_n*runs_cv_folds):\n",
    "                               \tfi_f.append(compute_feature_importance(clf[j], feat, fi[j].feat_sel, lp.learner_type))\n",
    "\n",
    "\t\t\tif not par['choose_cut']:\n",
    "\t\t\t\tif par['feature_identifier'] != 'UniRef90': \n",
    "\t\t\t\t\tsteps = [1,2,4,8,16,32,64] \n",
    "\t\t\t\telif par['feature_identifier'] == 'UniRef90': \n",
    "\t\t\t\t\tsteps = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096] \n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tsteps = list(map(int, par['choose_cut'].split(',')))\n",
    " \n",
    "\t\t\tif lp.learner_type == 'rf':\n",
    "\t\t\t\tfor k in steps:\n",
    "\t\t\t\t\tclf_f = []\n",
    "\t\t\t\t\tp_es_f = []\n",
    "\t\t\t\t\tl_es_f = []\n",
    "\n",
    "\t\t\t\t\tif lp.refine == 'rf':\t\n",
    "\t\t\t\t\t\tfor j in range(runs_n*runs_cv_folds):\n",
    "\t\t\t\t\t\t\tclf_f.append(\\\n",
    "\t\t\t\t\t\t\t\tRandomForestClassifier(n_estimators=par['number_of_trees']\n",
    "\t\t\t\t\t\t\t\t, criterion=par['rf_criterion']\\\n",
    "\t\t\t\t\t\t\t\t, max_features=(k if par['feature_identifier'] != 'UniRef90' else (k if k<=128 else 0.3))\\\n",
    "\t\t\t\t\t\t\t\t, max_depth=None\\\n",
    "\t\t\t\t\t\t\t\t, min_samples_split=2, oob_score=par['oob_score']\\\n",
    "\t\t\t\t\t\t\t\t, min_samples_leaf=par['number_sample_per_leaf']\\\n",
    "\t\t\t\t\t\t\t\t, n_jobs=par['ncores'], verbose=par['how_verbose']\\\n",
    "\t\t\t\t\t\t\t\t, class_weight='balanced')\\\n",
    "\t\t\t\t\t\t\t    .fit(f.loc[i_tr[j] & i_u[j//runs_cv_folds], fi_f[j].feat_sel[:k] ].values\\\n",
    "\t\t\t\t\t\t\t, l[i_tr[j] & i_u[j//runs_cv_folds]].values.flatten().astype('int')))\n",
    "\n",
    "\t\t\t\t\t\t\tp_es_f.append(pd.DataFrame(clf_f[j].predict_proba(f.loc[~i_tr[j] & i_u[j//runs_cv_folds]\\\n",
    "\t\t\t\t\t\t\t\t, fi_f[j].feat_sel[:k]].values)))\n",
    "\t\t\t\t\t\t\tl_es_f.append(pd.DataFrame([list(p_es_f[j].iloc[i,:]).index(max(p_es_f[j].iloc[i,:])) \\\n",
    "\t\t\t\t\t\t\tfor i in range(len(p_es_f[j]))]))\n",
    "\n",
    "\t\t\t\t\t\tcm_f = save_results(l if not par['objective_vector'] else l_ob, \\\n",
    "\t\t\t\t\t\t\tl_es_f, p_es_f, i_tr if not par['objective_vector'] else i_tr_ob, \\\n",
    "\t\t\t\t\t\t\ti_u, k, runs_n, runs_cv_folds)\n",
    "\n",
    "\t\t\t\t\t#elif lp.refine == 'svm':\t\t\t\t\n",
    "\t\t\t\t\t#\tfor j in range(runs_n*runs_cv_folds):\n",
    "\t\t\t\t\t#\t\tclf_f.append(GridSearchCV(\\\n",
    "\t\t\t\t\t#\t\t\tSVC(probability=True, verbose=1), lp.refine_grid, cv=StratifiedKFold(\\\n",
    "                                \t#\t\tl.iloc[i_tr[j] & i_u[j//runs_cv_folds],0], lp.cv_folds, shuffle=True)\\\n",
    "                                \t#\t\t\t, scoring=lp.cv_scoring, refit=True).fit(f.loc[i_tr[j] & i_u[j//runs_cv_folds]\\\n",
    "\t\t\t\t\t#\t\t\t, fi_f[j].feat_sel[:k]].values, l[i_tr[j] & i_u[j//runs_cv_folds]].values.flatten().astype('int')))\n",
    "\t\t\t\t\t#\t\tp_es_f.append(pd.DataFrame(clf_f[j].predict_proba(f.loc[~i_tr[j] & i_u[j//runs_cv_folds]\\\n",
    "\t\t\t\t\t#\t\t, fi_f[j].feat_sel[:k]].values)))\n",
    "\t\t\t\t\t#\t\tl_es_f.append(pd.DataFrame([list(p_es_f[j].iloc[i,:]).index(max(p_es_f[j].iloc[i,:])) \\\n",
    "\t\t\t\t\t#\t\tfor i in range(len(p_es_f[j]))]))\n",
    "\t\t\t\t\t#\tcm_f = save_results(l, l_es_f, p_es_f, i_tr, i_u, k, runs_n, runs_cv_folds)\n",
    " \n",
    "\t\t\tfi_ave = save_average_feature_importance(fi_f, feat)\n",
    " \n",
    "\tif par['out_f']:\n",
    "\t\tif lp.learner_type == 'rf':\n",
    "\t\t\tif not par['disable_features']:\n",
    "\t\t\t\tplot_pca(f, l if not par['objective_vector'] else l_ob, fi_ave.feat_sel)\n",
    " \n",
    "\t\tfidout.close()\n",
    "\t\tfidoutes.close()\n",
    "\t\tfidoutroc.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
